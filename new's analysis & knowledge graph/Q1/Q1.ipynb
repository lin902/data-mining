{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyOjPC8lUZ6KRrpBEf5klzAL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')"],"metadata":{"id":"2f9zldEXee3r","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1701755072437,"user_tz":-480,"elapsed":321248,"user":{"displayName":"卞liu","userId":"00044881131317824461"}},"outputId":"e0a8d9b0-9210-4a2a-fe52-fe1d66cd9db1"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["!pip install transformers\n"],"metadata":{"id":"7MjM1eNclx_B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import pandas as pd\n","import json\n","import numpy as np\n","from sklearn.metrics.pairwise import cosine_similarity\n","from transformers import AutoTokenizer, AutoModel"],"metadata":{"id":"29vJ0AUqfRfX","executionInfo":{"status":"ok","timestamp":1701755079908,"user_tz":-480,"elapsed":7498,"user":{"displayName":"卞liu","userId":"00044881131317824461"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# 加载News.xlsx文件\n","news_df = pd.read_excel('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/News.xlsx')\n"],"metadata":{"id":"_Uzy6bo71k9J","executionInfo":{"status":"ok","timestamp":1701755233202,"user_tz":-480,"elapsed":153318,"user":{"displayName":"卞liu","userId":"00044881131317824461"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["news_df.info"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixPp8K0uhC9p","executionInfo":{"status":"ok","timestamp":1701755245932,"user_tz":-480,"elapsed":532,"user":{"displayName":"卞liu","userId":"00044881131317824461"}},"outputId":"3f929b5a-a1ef-4f7d-9aa3-de6da6aadc42"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<bound method DataFrame.info of           NewsID                                    Title  \\\n","0              1                       建设银行原董事长张恩照一审被判15年   \n","1              2                             农行信用卡中心搬到上海滩   \n","2              3                        外运发展：价值型蓝筹股补涨要求强烈   \n","3              4                        胜利股份：稳步走强形成标准上升通道   \n","4              5               [港股快讯]恒指收市报18960点 成交467亿港元   \n","...          ...                                      ...   \n","1037030  1037031    亿华通：公司电解槽相关产品目前还处于产品的研发及测试阶段 尚未实现批量销售   \n","1037031  1037032                             依米康：接受中泰证券调研   \n","1037032  1037033         天风证券给予中核科技买入评级 核电行业景气上行 公司有望乘风而起   \n","1037033  1037034  海特生物：公司在抗癌药CPT获批后 会考虑适时开展CPT在海外的临床并谋求上市   \n","1037034  1037035                      恩捷股份：股东合益投资部分股份补充质押   \n","\n","                                               NewsContent NewsSource  \n","0        　　本报记者 田雨 李京华    　　中国建设银行股份有限公司原董事长张恩照受贿案３日一审宣...      中国证券报  \n","1        　　中国农业银行信用卡中心由北京搬到上海了！  　　农行行长杨明生日前在信用卡中心揭牌仪式上...       人民日报  \n","2        　　在新基金快速发行以及申购资金回流的情况下，市场总体上呈现资金流动性过剩格局，考虑到现阶段...      杭州新希望  \n","3        　　胜利股份（000407）公司子公司填海造地2800亩，以青岛的地价估算，静态价值在10亿...       源达投资  \n","4        　　全景网11月30日讯 外围股市造好，带动港股今早造好，恒指高开后反覆上升，最高升252点...        全景网  \n","...                                                    ...        ...  \n","1037030  每经AI快讯，有投资者在投资者互动平台提问：请问公司目前有没有电解槽产能，规划情况能否详细介...     每日经济新闻  \n","1037031  依米康（SZ 300249，收盘价：10.38元）发布公告称，2023年10月12日，依米康...     每日经济新闻  \n","1037032  天风证券10月13日发布研报称，给予中核科技（000777.SZ，最新价：13.03元）买入...     每日经济新闻  \n","1037033  有投资者提问：抗癌药CPT获批后，公司是否应该按照股权协议继续收购沙东股权，适应症为MM的C...       界面新闻  \n","1037034  10月13日午间，根据恩捷股份发布的公告，持有公司股份5%以上的股东玉溪合益投资有限公司（下...       证券日报  \n","\n","[1037035 rows x 4 columns]>"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["news_df.head(1000).to_csv('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/new_example.csv', index=False)"],"metadata":{"id":"v8EbVOGwVqw8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news_df_example = pd.read_csv('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/new_example.csv')"],"metadata":{"id":"wTroUG6LGdO6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_names = news_df_example.columns.tolist()\n","print(column_names)\n","news_df_example.loc[:1]"],"metadata":{"id":"5X6st2eCHuaM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ULvx9zFC3Sqw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["news_df_test = news_df_example[:10]\n","news_df_test"],"metadata":{"id":"O8CPgplhHRj_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 加载A_share_list.json文件\n","with open('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/A_share_list.json', 'r') as f:\n","    a_share_list = json.load(f)\n","\n","# 将A_share_list转换为DataFrame\n","a_share_df = pd.DataFrame(a_share_list)"],"metadata":{"id":"9JW6dgIBGXDC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["column_names = a_share_df.columns.tolist()\n","print(column_names)"],"metadata":{"id":"LLK3R0Pt2LNo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# bert-base-chinese"],"metadata":{"id":"0dM-TLZ-1V8G"}},{"cell_type":"code","source":["# 初始化BERT tokenizer和model\n","tokenizer = AutoTokenizer.from_pretrained('bert-base-chinese')\n","model = AutoModel.from_pretrained('bert-base-chinese')"],"metadata":{"id":"ihXZXKDJ1En5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 对公司简名进行向量化\n","company_name = []\n","for company in a_share_df['name']:\n","    encoded_input = tokenizer(company, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","    with torch.no_grad():\n","        model_output = model(**encoded_input)\n","    company_vector = model_output.pooler_output.numpy()[0]\n","    company_name.append(company_vector)\n","\n","company_name = np.stack(company_name) #列表转换为NumPy数组\n"],"metadata":{"id":"naNphXNJGsE1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 对公司全名进行向量化\n","company_fullname = []\n","for company in a_share_df['fullname']:\n","    company = company.replace(\"股份有限公司\", \"\").replace(\"集团\", \"\")\n","    encoded_input = tokenizer(company, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","    with torch.no_grad():\n","        model_output = model(**encoded_input)\n","    company_vector = model_output.pooler_output.numpy()[0]\n","    company_fullname.append(company_vector)\n","\n","company_fullname = np.stack(company_fullname) #列表转换为NumPy数组"],"metadata":{"id":"LRUccQzwm0eL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["试错，后删"],"metadata":{"id":"ub0i83aT2iZf"}},{"cell_type":"code","source":["import re\n","import jieba\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk import pos_tag\n","\n","# 下载停用词表\n","nltk.download('stopwords')\n","nltk.download('averaged_perceptron_tagger')\n","# 加载停用词表\n","stopwords = set(stopwords.words('chinese'))  # 使用NLTK内置的中文停用词表\n","\n","# 需要去除的特定词汇列表\n","specific_words = [\"股份有限公司\", \"集团\"]\n","\n","def preprocess(text):\n","    # 去除特殊字符和标点符号\n","    text = re.sub(r\"[^\\w\\s]\", \"\", text)\n","\n","    # 去除多余的空格和换行符\n","    text = re.sub(r\"\\s+\", \" \", text).strip()\n","\n","\n","    # 分词\n","    words = jieba.lcut(text)\n","\n","    # 去除停用词\n","    words = [word for word in words if word not in stopwords]\n","\n","    # 词性标注\n","    tagged_words = pos_tag(words)\n","\n","    # 过滤名词\n","    filtered_nouns = [word for word, pos in tagged_words if pos.startswith('N') and word not in specific_words]\n","    filtered_nouns = [noun for noun in filtered_nouns if noun.strip() != '']\n","\n","    return filtered_nouns"],"metadata":{"id":"IGYPSdnU09yv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["正确的预处理"],"metadata":{"id":"U35UwEMY2lm3"}},{"cell_type":"code","source":["!pip install thulac"],"metadata":{"id":"R_skhWHBlCFy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import re\n","import thulac\n","import nltk\n","from nltk.corpus import stopwords\n","\n","# 下载停用词表\n","nltk.download('stopwords')\n","# 加载停用词表\n","stopwords = set(stopwords.words('chinese'))  # 使用NLTK内置的中文停用词表\n","\n","# 需要去除的特定词汇列表\n","specific_words = [\"股份\",\"有限公司\", \"集团\"]  # 添加你想去除的特定词汇\n"],"metadata":{"id":"_tMS41x2ltyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def preprocess(text):\n","    # 初始化THULAC实例\n","\n","    thu = thulac.thulac()\n","\n","    # 分词和词性标注\n","    result = thu.cut(text, text=True)\n","\n","\n","\n","     # 过滤名词\n","    nouns = []\n","\n","    for word_pos in result.split():\n","        word, pos = word_pos.split('_')\n","        if pos == 'n' and word not in specific_words and word not in stopwords:\n","            nouns.append(word)\n","\n","    return nouns\n","\n","# 测试\n","#news_content = \"本报记者 田雨 李京华 　　中国建设银行股份有限公司原董事长张恩照受贿案３日一审宣判，北京市第一中级人民法院依法以受贿罪判处张恩照有期徒刑１５年。 　　法院经开庭审理查明，２０００年至２００４年期间，被告人张恩照利用其担任原中国建设银行副行长、行长，中国建设银行股份有限公司董事长的职务便利，为他人牟取利益，多次非法收受他人给予的款物共计人民币４００余万元。案发后，赃款、赃物已全部退缴。 　　法院认为，被告人张恩照身为国家工作人员，利用职务上的便利，为他人谋取利益，非法收受他人财物，其行为已构成受贿罪，受贿数额特别巨大。鉴于张恩照因其他违纪问题被审查后，主动交代了有关部门不掌握的本案受贿事实，属于自首，且赃款、赃物已全部退缴，对张恩照依法可从轻处罚。法院遂依法以受贿罪判处张恩照有期徒刑１５年。\"\n","#filtered_nouns = preprocess(news_content)\n","#print(\"\".join(filtered_nouns))"],"metadata":{"id":"09B_ChT7kvKi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# 定义阈值，阈值和新闻文本中的噪音长度有关\n","threshold = 0.8\n","# 存储有效新闻的列表\n","valid_news = []\n","\n","# 遍历新闻条目\n","for index, news_row in news_df_test.iterrows():\n","    # 提取新闻\n","    newsID = news_row['NewsID']\n","    news_title = news_row['Title'] #\n","    news_content = news_row['NewsContent']\n","\n","    filtered_nouns = preprocess(news_content)\n","\n","    content = news_title+\"\".join(filtered_nouns)\n","\n","\n","\n","    # 将新闻向量化\n","    encoded_input = tokenizer(content, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","\n","    name = ['建设银行']\n","    answer = tokenizer(name, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","    #tokens = tokenizer.tokenize(content)\n","    #token_count = len(tokens)\n","    #print(\"Token count:\", token_count)\n","\n","    with torch.no_grad():\n","        model_output = model(**encoded_input)\n","    news_vector = model_output.pooler_output.numpy()[0]\n","    #print(news_vector.shape) (768,)\n","    with torch.no_grad():\n","        model_output = model(**answer)\n","    answer_vector = model_output.pooler_output.numpy()[0]\n","    #print(news_vector.shape) (768,)\n","    # 计算新闻向量与公司向量之间的相似度\n","    #company_vectors.shape (4654, 768)\n","    similarities = cosine_similarity(news_vector.reshape(1, -1), company_name)\n","    #print(similarities.shape) (1, 4654)\n","\n","\n","    similarity_a = cosine_similarity(news_vector.reshape(1, -1), answer_vector.reshape(1, -1))\n","    print(\"正确答案的相似度：\",similarity_a)\n","    # 检查是否有相似度超过阈值的公司\n","    # 获取所有相似度大于阈值的公司索引\n","    max_similarity_index = np.argmax(similarities)\n","    similar_indices = np.where(similarities[0, :] > threshold)[0]\n","    print(similar_indices.size)\n","\n","\n","\n","\n","\n","    if similar_indices.size > 0:\n","\n","      max_similarity = similarities[:,max_similarity_index]\n","      min_similarity = np.min(similarities[0, similar_indices])\n","      print(\"最大相似度：\", max_similarity)\n","      print(\"最小相似度：\", min_similarity)\n","      #print(similarities.shape)           # 打印相似度数组的形状(1, 4654)\n","      #print(similar_indices.shape)        # 打印索引数组的形状(12,)\n","\n","      result_df = pd.DataFrame({\n","          'Similarity': similarities[0,similar_indices],\n","          'Company Name': a_share_df.loc[similar_indices, 'name']\n","      })\n","\n","      print(result_df)\n","\n","      # 获取所有相似度大于阈值的公司名称\n","      Explicit_companies = a_share_df.loc[similar_indices, 'name'].tolist()\n","      #Explicit_companies = a_share_df.loc[max_similarity_index, 'name']\n","      valid_news.append((newsID,content,Explicit_companies))\n","\n","\n","# 将有效新闻转换为DataFrame\n","valid_news_df = pd.DataFrame(valid_news)"],"metadata":{"id":"uhBKZaQbGzXN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ODCUmARqzhyS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data = {\n","    'Index': [0],\n","    'NewsID': [1],\n","    'Title': ['建设银行'],\n","    'NewsContent': ['建设银行原董事长张恩照一审被判15年记者田雨银行董事长受贿案一审受贿罪法院被告人银行行长行长银行董事长职务利益款物人民币赃款赃物法院被告人国家人员职务利益财物受贿罪数额部门事实赃款赃物法院受贿罪'],\n","    'NewsSource': ['中国证券报']\n","}\n","\n","news_df_test = pd.DataFrame(data)"],"metadata":{"id":"VUO_rZu9zsWx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nZMWPakI4GJr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["valid_news_df"],"metadata":{"id":"j7xuM1XpIqFX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kvqLqPXwvs__"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# BERT-wwm"],"metadata":{"id":"taHSYCWx1OEq"}},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, AutoModel"],"metadata":{"id":"RTqPBvLF7AuC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 加载tokenizer和模型\n","tokenizer = AutoTokenizer.from_pretrained('hfl/chinese-bert-wwm')\n","model = AutoModel.from_pretrained('hfl/chinese-bert-wwm')\n","\n","# 对公司简名进行向量化\n","company_name = []\n","for company in a_share_df['name']:\n","  # 使用tokenizer对文本进行分词和编码\n","  tokens = tokenizer.encode_plus(company,add_special_tokens=True,padding='max_length',truncation=True,max_length=128,return_tensors='pt')\n","  # 获取模型的输入张量\n","  input_ids = tokens['input_ids']\n","  attention_mask = tokens['attention_mask']\n","\n","  # 使用模型进行向量化\n","  with torch.no_grad():\n","      outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n","\n","  # 获取文本的向量表示\n","  company_vector = outputs.last_hidden_state.mean(dim=1).squeeze()\n","  company_name = [company_vector.detach().numpy() for vector in company_vector]\n","  company_name = np.stack(company_name)\n","\n","\n"],"metadata":{"id":"DvfD6cHO6-Go"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 定义阈值，阈值和新闻文本中的噪音长度有关\n","threshold = 0.8\n","# 存储有效新闻的列表\n","valid_news = []\n","\n","# 遍历新闻条目\n","for index, news_row in news_df_test.iterrows():\n","    # 提取新闻\n","    newsID = news_row['NewsID']\n","    news_title = news_row['Title'] #\n","    news_content = news_row['NewsContent']\n","\n","    filtered_nouns = preprocess(news_content)\n","\n","    content = news_title+\"\".join(filtered_nouns)\n","\n","\n","\n","    # 将新闻向量化\n","    tokens = tokenizer.encode_plus(content,add_special_tokens=True,padding='max_length',truncation=True,max_length=128,return_tensors='pt')\n","    encoded_input = tokenizer(content, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","\n","    name = ['建设银行']\n","    answer = tokenizer(name, padding=True, truncation=True, max_length=128, return_tensors='pt')\n","    #tokens = tokenizer.tokenize(content)\n","    #token_count = len(tokens)\n","    #print(\"Token count:\", token_count)\n","\n","    with torch.no_grad():\n","        model_output = model(**encoded_input)\n","    news_vector = model_output.pooler_output.numpy()[0]\n","    #print(news_vector.shape) (768,)\n","    with torch.no_grad():\n","        model_output = model(**answer)\n","    answer_vector = model_output.pooler_output.numpy()[0]\n","    #print(news_vector.shape) (768,)\n","    # 计算新闻向量与公司向量之间的相似度\n","    #company_vectors.shape (4654, 768)\n","    similarities = cosine_similarity(news_vector.reshape(1, -1), company_name)\n","    #print(similarities.shape) (1, 4654)\n","\n","\n","    similarity_a = cosine_similarity(news_vector.reshape(1, -1), answer_vector.reshape(1, -1))\n","    print(\"正确答案的相似度：\",similarity_a)\n","    # 检查是否有相似度超过阈值的公司\n","    # 获取所有相似度大于阈值的公司索引\n","    max_similarity_index = np.argmax(similarities)\n","    similar_indices = np.where(similarities[0, :] > threshold)[0]\n","    print(similar_indices.size)\n","\n","\n","\n","\n","\n","    if similar_indices.size > 0:\n","\n","      max_similarity = similarities[:,max_similarity_index]\n","      min_similarity = np.min(similarities[0, similar_indices])\n","      print(\"最大相似度：\", max_similarity)\n","      print(\"最小相似度：\", min_similarity)\n","      #print(similarities.shape)           # 打印相似度数组的形状(1, 4654)\n","      #print(similar_indices.shape)        # 打印索引数组的形状(12,)\n","\n","      result_df = pd.DataFrame({\n","          'Similarity': similarities[0,similar_indices],\n","          'Company Name': a_share_df.loc[similar_indices, 'name']\n","      })\n","\n","      print(result_df)\n","\n","      # 获取所有相似度大于阈值的公司名称\n","      Explicit_companies = a_share_df.loc[similar_indices, 'name'].tolist()\n","      #Explicit_companies = a_share_df.loc[max_similarity_index, 'name']\n","      valid_news.append((newsID,content,Explicit_companies))\n","\n","\n","# 将有效新闻转换为DataFrame\n","valid_news_df = pd.DataFrame(valid_news)"],"metadata":{"id":"ld2Xcg1x1_mg"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 关键词匹配"],"metadata":{"id":"1T_3gX7MEjZO"}},{"cell_type":"code","source":["import pandas as pd\n","import pickle\n","import math\n","# 公司名称列表，假设为一个包含公司名称和公司全名的列表\n","company_name = a_share_df['name']\n","company_fullname = a_share_df['fullname']\n","\n","# 定义检查点文件路径\n","checkpoint_file = '/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/checkpoint/filtered_news_checkpoint.pkl'\n","# 定义保存检查点的频率\n","save_checkpoint_freq = 10000\n","\n","# 记录已处理的新闻数量\n","processed_news_count = 0\n","\n","# 检查是否存在检查点文件\n","try:\n","    # 尝试加载检查点文件\n","    with open(checkpoint_file, 'rb') as f:\n","        filtered_df = pickle.load(f)\n","except FileNotFoundError:\n","    # 创建新的数据框用于存储包含公司名的新闻\n","    filtered_df = pd.DataFrame(columns=['NewsID', 'NewsContent', 'Explicit_Company'])\n","\n","# 遍历每条新闻文本\n","for index, row in news_df.iterrows():\n","    newsID = row['NewsID']\n","    news_title = row['Title']\n","    news_content = row['NewsContent']\n","\n","    # 跳过包含缺失值的新闻内容\n","    if isinstance(news_content, float) and math.isnan(news_content):\n","        continue\n","\n","    # 在新闻文本中寻找匹配公司名称的部分\n","    explicit_company = []\n","\n","    # 在公司名称列表中进行关键词匹配\n","    for name, fullname in zip(company_name, company_fullname):\n","        if name in news_content or fullname in news_content:\n","            explicit_company.append(name)\n","\n","    if explicit_company:\n","        # 找到匹配的公司名称，将新闻添加到新的数据框中\n","        filtered_df = pd.concat([filtered_df, pd.DataFrame({'NewsID': [newsID], 'NewsContent': [news_content], 'Explicit_Company': [explicit_company]})], ignore_index=True)\n","\n","    # 增加已处理的新闻数量\n","    processed_news_count += 1\n","\n","    # 当达到保存检查点的频率时，保存检查点\n","    if processed_news_count % save_checkpoint_freq == 0:\n","        with open(checkpoint_file, 'wb') as f:\n","            pickle.dump(filtered_df, f)\n","\n","# 最后一次保存检查点\n","with open(checkpoint_file, 'wb') as f:\n","    pickle.dump(filtered_df, f)\n","\n","# 打印筛选后的新闻数据框\n","#print(filtered_df)\n","\n","# 保存有效新闻到新的文档\n","filtered_df.to_excel('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/Q1/filtered_news_1.xlsx', index=False)"],"metadata":{"id":"TvY9rdoaNNiK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","# 公司名称列表，假设为一个包含公司名称和公司全名的列表\n","company_name = a_share_df['name']\n","company_fullname = a_share_df['fullname']\n","\n","# 创建新的数据框用于存储包含公司名的新闻\n","filtered_df = pd.DataFrame(columns=['NewsID','NewsContent','Explicit_Company'])\n","\n","# 遍历每条新闻文本\n","for index, row in news_df_example.iterrows():\n","\n","    newsID = row['NewsID']\n","    news_title = row['Title']\n","    news_content = row['NewsContent']\n","\n","    # 在新闻文本中寻找匹配公司名称的部分\n","    explicit_company = []\n","\n","    # 在公司名称列表中进行关键词匹配\n","    for name, fullname in zip(company_name, company_fullname):\n","        if name in news_content or fullname in news_content:\n","            explicit_company.append(name)\n","\n","    if explicit_company:\n","        # 找到匹配的公司名称，将新闻添加到新的数据框中\n","        filtered_df = pd.concat([filtered_df, pd.DataFrame({'NewsID': [newsID], 'NewsContent': [news_content], 'Explicit_Company': [explicit_company]})], ignore_index=True)\n","\n","\n","# 打印筛选后的新闻数据框\n","#print(filtered_df)\n","# 保存有效新闻到新的文档\n","filtered_df.to_excel('/content/drive/MyDrive/Colab_Notebooks/5002/dsaa5002_project/Q1/filtered_news_1.xlsx', index=False)"],"metadata":{"id":"WLNpvK0YEobr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"QEzIiX475eCL"},"execution_count":null,"outputs":[]}]}